<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>AI on a laptop | Arian's blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The AI craze is going strong. You would think it&rsquo;d be over by now, but
big boy companies keep throwing their money at Nvidia GPUs and
overpriced LLM subscriptions.
For the average Joe, GPT4 and GPT4o are more than enough to do your
day-to-day tasks or if you&rsquo;re a developer, you can just have OpenAI&rsquo;s
API do everything you&rsquo;re too lazy to do. This all comes at a cost:

Data privacy
Are you okay sending your or your company&rsquo;s data to
OpenAI&rsquo;s servers, even if they don&rsquo;t use it to train their future model?
Security
Is your data going to be secure on their servers? It&rsquo;s
not like they&rsquo;ve ever had a breach.
Censorship
Did Sam Altman ask the underpaid kenyan workers to not
let you or the LLM talk about certain things?

All of these concerns are valid, and while personally I don&rsquo;t care
about certain conversations being leaked or being publicly attributed
to me, I do care about the 3rd point."><meta name=generator content="Hugo 0.150.1"><meta name=robots content="index, follow"><link rel=stylesheet href=/blog/ananke/css/main.min.2438bcafd7af9675c426d1a4afcd16cfff18e4e10f401071e45b5ccd3be40a0d.css><link rel=canonical href=http://arian-d.github.io/blog/posts/ai-on-a-laptop/><meta property="og:url" content="http://arian-d.github.io/blog/posts/ai-on-a-laptop/"><meta property="og:site_name" content="Arian's blog"><meta property="og:title" content="AI on a laptop"><meta property="og:description" content="The AI craze is going strong. You would think it’d be over by now, but big boy companies keep throwing their money at Nvidia GPUs and overpriced LLM subscriptions.
For the average Joe, GPT4 and GPT4o are more than enough to do your day-to-day tasks or if you’re a developer, you can just have OpenAI’s API do everything you’re too lazy to do. This all comes at a cost:
Data privacy Are you okay sending your or your company’s data to OpenAI’s servers, even if they don’t use it to train their future model? Security Is your data going to be secure on their servers? It’s not like they’ve ever had a breach. Censorship Did Sam Altman ask the underpaid kenyan workers to not let you or the LLM talk about certain things? All of these concerns are valid, and while personally I don’t care about certain conversations being leaked or being publicly attributed to me, I do care about the 3rd point."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-29T00:00:00+00:00"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Llama"><meta property="article:tag" content="Thinkpad"><meta property="article:tag" content="Podman"><meta itemprop=name content="AI on a laptop"><meta itemprop=description content="The AI craze is going strong. You would think it’d be over by now, but big boy companies keep throwing their money at Nvidia GPUs and overpriced LLM subscriptions.
For the average Joe, GPT4 and GPT4o are more than enough to do your day-to-day tasks or if you’re a developer, you can just have OpenAI’s API do everything you’re too lazy to do. This all comes at a cost:
Data privacy Are you okay sending your or your company’s data to OpenAI’s servers, even if they don’t use it to train their future model? Security Is your data going to be secure on their servers? It’s not like they’ve ever had a breach. Censorship Did Sam Altman ask the underpaid kenyan workers to not let you or the LLM talk about certain things? All of these concerns are valid, and while personally I don’t care about certain conversations being leaked or being publicly attributed to me, I do care about the 3rd point."><meta itemprop=datePublished content="2024-05-29T00:00:00+00:00"><meta itemprop=dateModified content="2024-05-29T00:00:00+00:00"><meta itemprop=wordCount content="597"><meta itemprop=keywords content="Llm,Llama,Thinkpad,Podman"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI on a laptop"><meta name=twitter:description content="The AI craze is going strong. You would think it’d be over by now, but big boy companies keep throwing their money at Nvidia GPUs and overpriced LLM subscriptions.
For the average Joe, GPT4 and GPT4o are more than enough to do your day-to-day tasks or if you’re a developer, you can just have OpenAI’s API do everything you’re too lazy to do. This all comes at a cost:
Data privacy Are you okay sending your or your company’s data to OpenAI’s servers, even if they don’t use it to train their future model? Security Is your data going to be secure on their servers? It’s not like they’ve ever had a breach. Censorship Did Sam Altman ask the underpaid kenyan workers to not let you or the LLM talk about certain things? All of these concerns are valid, and while personally I don’t care about certain conversations being leaked or being publicly attributed to me, I do care about the 3rd point."></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/blog/ class="f3 fw2 hover-white no-underline white-90 dib">Arian's blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">AI on a laptop</h1><time class="f6 mv4 dib tracked" datetime=2024-05-29T00:00:00Z>May 29, 2024</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>The AI craze is going strong. You would think it&rsquo;d be over by now, but
big boy companies keep throwing their money at Nvidia GPUs and
overpriced LLM subscriptions.</p><p>For the average Joe, GPT4 and GPT4o are more than enough to do your
day-to-day tasks or if you&rsquo;re a developer, you can just have OpenAI&rsquo;s
API do everything you&rsquo;re too lazy to do. This all comes at a cost:</p><dl><dt>Data privacy</dt><dd>Are you okay sending your or your company&rsquo;s data to
OpenAI&rsquo;s servers, even if they don&rsquo;t use it to train their future model?</dd><dt>Security</dt><dd>Is your data going to be secure on their servers? <a href=https://securityintelligence.com/articles/chatgpt-confirms-data-breach/>It&rsquo;s
not like they&rsquo;ve ever had a breach</a>.</dd><dt>Censorship</dt><dd>Did Sam Altman ask the <a href=https://time.com/6247678/openai-chatgpt-kenya-workers/>underpaid kenyan workers</a> to not
let you or the LLM talk about certain things?</dd></dl><p>All of these concerns are valid, and while personally I don&rsquo;t care
about certain conversations being leaked or being publicly attributed
to me, I do care about the 3rd point.</p><ul><li>What if I&rsquo;m trying to get the command to exploit a server I own? I&rsquo;d
have to &ldquo;jailbreak&rdquo; GPT to convince it and that I&rsquo;m doing it ethically.</li><li>What if I want to curse to relieve stress? What if I want GPT to
write a story and have it use bad words? I&rsquo;d have to bend over
backward just to get a simple thing out of it.</li><li>What if some of the big boy companies (or even governments) decide
to ban the use of LLMs? Well, you know how that would go&mldr;</li></ul><p>You catch my drift, right? There are similar arguments for FOSS
vs SaaS software. The difference in LLMs is that up until recently,
you couldn&rsquo;t have a self-hosted AI waifu hallucinating on your PC
unless you had sold a kidney or two to buy a used Nvidia card.</p><p>Luckily, thanks to some very smart people, there now exists the
possibility of running these models, on your PC with just a decent CPU
and some RAM.</p><p>I recently got <a href=https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadp/thinkpad-p14s-gen-4-(14-inch-amd)-mobile-workstation/21k5001jus>a new thinkpad</a><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> that came out last year and I luckily
managed to buy it used from a 3rd party seller on Amazon. It&rsquo;s a beefy
beast. It has</p><ul><li>An 8 core / 16 thread CPU, going up to 5-something GHz</li><li>2k OLED display</li><li>and a cherry on top, which is 64G of LPDDR5X RAM.</li></ul><p>Despite not having a dedicated GPU, and me not using the APU&rsquo;s <a href=https://www.amd.com/en/products/software/rocm.html>ROCm</a>,
I can still run models fairly fast<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>Initially, I had tried <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>, and it is still my go-to because of
the <a href=https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md>grammar feature</a>. Recently, I&rsquo;ve been using <a href=https://ollama.com/>ollama</a>, which is a bit
extra for what I&rsquo;m doing, but it has a nice and easy interface for
chatting. It also has <a href=https://ollama.com/blog/openai-compatibility>an API</a> that matches <a href=https://github.com/openai/openai-openapi>OpenAI&rsquo;s API</a> more closely
than <a href=https://github.com/ggerganov/llama.cpp/tree/master/examples/server>llama.cpp&rsquo;s server</a>.</p><p>To share my daily process, I generally use podman to start a container
with a named volume</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>podman run --rm -d --name ollama -v ollama:/root/.ollama -p 11434:11434 docker.io/ollama/ollama
</span></span></code></pre></div><p>and whenever I want to chat, I run something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>podman exec -it ollama ollama run dolphin-llama3:70b
</span></span></code></pre></div><p>The pulling mehcanism is similar to docker/podman where the &ldquo;model&rdquo; is
pulled if it doesn&rsquo;t exist, and if it does, ollama will drop you in a
chat interface.</p><p>If you use Emacs, <a href=https://github.com/karthink/gptel>gptel</a> is also a nice touch for not escaping the
world&rsquo;s best operating system.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Phoronix did <a href=https://www.phoronix.com/review/thinkpad-p14s-gen4>a review</a> of it.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>I use <a href=https://wiki.gentoo.org/wiki/Power_management/Processor#cpupower>cpupower on Gentoo</a> and set the frequency to use the
<code>performance</code> profile. I have no idea if that&rsquo;s the best way, but it
seems to speed things up based the usage I see on <a href=https://github.com/aristocratos/btop>btop</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><ul class=pa0><li class="list di"><a href=/blog/tags/llm/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Llm</a></li><li class="list di"><a href=/blog/tags/llama/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Llama</a></li><li class="list di"><a href=/blog/tags/thinkpad/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Thinkpad</a></li><li class="list di"><a href=/blog/tags/podman/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Podman</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://arian-d.github.io/blog/>&copy; Arian's blog 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>